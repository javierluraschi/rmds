"url","code"
"https://raw.githubusercontent.com/statkclee/nlp2/22e182a631e53b6a97e3d8c6937b1db6ede0bfa3/nlp-tensorflow-install.Rmd","knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE,
                      comment="""", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')

library(reticulate)
use_condaenv(""anaconda3"")
# reticulate::repl_python()

name: cpu-tf
dependencies:
  - python=3.6  
  - jupyter  
  - ipython  
  - pandas
"
"https://raw.githubusercontent.com/cran/tfdeploy/391ed12acccd64a9c441d7c5f4902c851fa69cfb/inst/doc/introduction.Rmd","knitr::opts_chunk$set(eval = FALSE)

install.packages(tfdeploy)

library(keras)

# load data
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()

# reshape and rescale
x_train <- array_reshape(x_train, dim = c(nrow(x_train), 784)) / 255
x_test <- array_reshape(x_test, dim = c(nrow(x_test), 784)) / 255

# one-hot encode response
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

# define and compile model
model <- keras_model_sequential()
model %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(784),
              name = ""image"") %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax',
              name = ""prediction"") %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
  )

# train model
history <- model %>% fit(
  x_train, y_train,
  epochs = 35, batch_size = 128,
  validation_split = 0.2
)

preds <- predict(model, x_test[1:5,])

library(tfdeploy)
export_savedmodel(model, ""savedmodel"")

view_savedmodel(""savedmodel"")

library(tfdeploy)
serve_savedmodel('savedmodel', browse = TRUE)

library(cloudml)
cloudml_deploy(""savedmodel"", name = ""keras_mnist"", version = ""keras_mnist_1"")

export_savedmodel(model, ""savedmodel"")

library(tfestimators)

mtcars_input_fn <- function(data, num_epochs = 1) {
  input_fn(data,
           features = c(""disp"", ""cyl""),
           response = ""mpg"",
           batch_size = 32,
           num_epochs = num_epochs)
}

cols <- feature_columns(column_numeric(""disp""), column_numeric(""cyl""))

model <- linear_regressor(feature_columns = cols)

indices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))
train <- mtcars[indices, ]
test  <- mtcars[-indices, ]

model %>% train(mtcars_input_fn(train, num_epochs = 10))

export_savedmodel(model, ""savedmodel"")

library(tensorflow)

sess <- tf$Session()
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets(""MNIST-data"", one_hot = TRUE)

# Note that we define x as the input tensor
# and y as the output tensor that will contain
# the scores. These are referenced in export_savedmodel
x <- tf$placeholder(tf$float32, shape(NULL, 784L))
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))
y <- tf$nn$softmax(tf$matmul(x, W) + b)
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
cross_entropy <- tf$reduce_mean(
  -tf$reduce_sum(y_ * tf$log(y), reduction_indices=1L)
)

optimizer <- tf$train$GradientDescentOptimizer(0.5)
train_step <- optimizer$minimize(cross_entropy)

init <- tf$global_variables_initializer()
sess$run(init)

for (i in 1:1000) {
  batches <- mnist$train$next_batch(100L)
  batch_xs <- batches[[1]]
  batch_ys <- batches[[2]]
  sess$run(train_step,
           feed_dict = dict(x = batch_xs, y_ = batch_ys))
}

export_savedmodel(
  sess,
  ""savedmodel"",
  inputs = list(image_input = x),
  outputs = list(scores = y))

library(cloudml)
cloudml_deploy(""savedmodel"", name = ""keras_mnist"")
"
"https://raw.githubusercontent.com/cran/tfdeploy/391ed12acccd64a9c441d7c5f4902c851fa69cfb/vignettes/introduction.Rmd","knitr::opts_chunk$set(eval = FALSE)

install.packages(tfdeploy)

library(keras)

# load data
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()

# reshape and rescale
x_train <- array_reshape(x_train, dim = c(nrow(x_train), 784)) / 255
x_test <- array_reshape(x_test, dim = c(nrow(x_test), 784)) / 255

# one-hot encode response
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)

# define and compile model
model <- keras_model_sequential()
model %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(784),
              name = ""image"") %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax',
              name = ""prediction"") %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
  )

# train model
history <- model %>% fit(
  x_train, y_train,
  epochs = 35, batch_size = 128,
  validation_split = 0.2
)

preds <- predict(model, x_test[1:5,])

library(tfdeploy)
export_savedmodel(model, ""savedmodel"")

view_savedmodel(""savedmodel"")

library(tfdeploy)
serve_savedmodel('savedmodel', browse = TRUE)

library(cloudml)
cloudml_deploy(""savedmodel"", name = ""keras_mnist"", version = ""keras_mnist_1"")

export_savedmodel(model, ""savedmodel"")

library(tfestimators)

mtcars_input_fn <- function(data, num_epochs = 1) {
  input_fn(data,
           features = c(""disp"", ""cyl""),
           response = ""mpg"",
           batch_size = 32,
           num_epochs = num_epochs)
}

cols <- feature_columns(column_numeric(""disp""), column_numeric(""cyl""))

model <- linear_regressor(feature_columns = cols)

indices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))
train <- mtcars[indices, ]
test  <- mtcars[-indices, ]

model %>% train(mtcars_input_fn(train, num_epochs = 10))

export_savedmodel(model, ""savedmodel"")

library(tensorflow)

sess <- tf$Session()
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets(""MNIST-data"", one_hot = TRUE)

# Note that we define x as the input tensor
# and y as the output tensor that will contain
# the scores. These are referenced in export_savedmodel
x <- tf$placeholder(tf$float32, shape(NULL, 784L))
W <- tf$Variable(tf$zeros(shape(784L, 10L)))
b <- tf$Variable(tf$zeros(shape(10L)))
y <- tf$nn$softmax(tf$matmul(x, W) + b)
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))
cross_entropy <- tf$reduce_mean(
  -tf$reduce_sum(y_ * tf$log(y), reduction_indices=1L)
)

optimizer <- tf$train$GradientDescentOptimizer(0.5)
train_step <- optimizer$minimize(cross_entropy)

init <- tf$global_variables_initializer()
sess$run(init)

for (i in 1:1000) {
  batches <- mnist$train$next_batch(100L)
  batch_xs <- batches[[1]]
  batch_ys <- batches[[2]]
  sess$run(train_step,
           feed_dict = dict(x = batch_xs, y_ = batch_ys))
}

export_savedmodel(
  sess,
  ""savedmodel"",
  inputs = list(image_input = x),
  outputs = list(scores = y))

library(cloudml)
cloudml_deploy(""savedmodel"", name = ""keras_mnist"")
"
"https://raw.githubusercontent.com/asbates/asbates/cc7effd40884c41790f09d5f3afa2f264b049060/static/slides/keras-intro.Rmd","options(htmltools.dir.version = FALSE)

knitr::include_graphics(""keras-logo-2018-large-1200.png"")

knitr::include_graphics(""landing_icon-tf_logo_halo_480.jpg"")

library(keras)
library(dplyr)  # for the %>% 

model <- keras_model_sequential() %>% # initialize a model
  layer_dense(units = 10, 
              activation = ""sigmoid"",
              input_shape = p) %>% # add a hidden layer
  layer_dense(units = 1) # add an output layer

# set things up for training
model %>% compile(
  loss = ""mse"",
  optimizer = optimizer_sgd(),
  metrics = ""mse""
)

# train the model
model %>% fit(
  x_train,
  y_train
)

"
"https://raw.githubusercontent.com/rstudio/tensorflow-blog/3437f12f7869999a520f06fc402e8d877c021a71/_posts/2017-08-17-tensorflow-v13-released/tensorflow-v13-released.Rmd","  knitr::opts_chunk$set(echo = FALSE)
"
"https://raw.githubusercontent.com/nanjekyejoannah/deep_learning_in_containers/af808a1412fde2a59e33b1e471c5598a906e0348/containerized_deep_learning.Rmd",""
"https://raw.githubusercontent.com/rstudio/rstudio-conf/a2607189c6e0a6c453b205dba213e47d507b35bd/2018/Deploying_TensorFlow_Models--Javier%20Luraschi/deploy-tensorflow-models-with-tfdeploy.Rmd","options(htmltools.dir.version = FALSE)

# Install tfdeploy from GitHub
devtools::install_github(""rstudio/tfdeploy"")

# Run local server with model
serve_savedmodel(""<model-name>"")

# Deploy to CloudML
library(cloudml)
cloudml_deploy(""keras-mnist"", ""rstudio-conf"")

# Deploy to RStudio Connect
library(rsconnect)
deployTFModel(""keras-mnist"")

# Install kerasjs from GitHub
devtools::install_github(""rstudio/kerasjs"")

# Train and Export model from Keras as HDF5
# or use an existing model
model_path <- system.file(
  ""models/keras-mnist.hdf5"",
  package = ""kerasjs""
)

# Convert model to JavaScript and Preview
kerasjs_convert(model_path)

data <- list(instances = list(
    list(pixels::get_pixels())
))

# Request prediction from deployed model
httr::POST(url, body = toJSON(data))

# Or directly from TensorFlow
tfdeploy::predict_savedmodel(data)

# Then assign to reactive, etc.

mtcars_tbl %>% spark_apply(function(df) {
  instances <- unname(apply(df, 1, function(e) 
    list(cyl = e[2], disp = e[3])
  ))
  
  results <- tfdeploy::predict_savedmodel(
    instances,
    ""tfestimators-mtcars.tar"",
    signature_name = ""predict""
  )

  unname(unlist(results))
})
"
"https://raw.githubusercontent.com/skydome20/R-Notes/a7b0be81ad1b0bff6e52df0aa7eb3f84afe8b89d/src/R15/R15.Rmd","# installing 'devtools' package for installing Packages from github
install.packages('devtools')

require(devtools)

# install tensorflow(如果你要tensorflow的話)
devtools::install_github(""rstudio/tensorflow"") 

# installing keras(如果你要keras的話)
devtools::install_github(""rstudio/keras"") 

# Test Tensorflow: Say hello
require(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)

# Test Keras: Define a simple DNN network
require(keras)
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = ""relu"", input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = ""relu"") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = ""softmax"")

summary(model)

sessionInfo()
packageVersion(""tensorflow"")
packageVersion(""keras"")
"
"https://raw.githubusercontent.com/rstudio/tensorflow-blog/3437f12f7869999a520f06fc402e8d877c021a71/_posts/2017-08-31-tensorflow-estimators-for-r/tensorflow-estimators-for-r.Rmd","  knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

devtools::install_github(""rstudio/tfestimators"")

library(tfestimators)
install_tensorflow()

library(tfestimators)

# return an input_fn for a given subset of data
mtcars_input_fn <- function(data) {
  input_fn(data, 
           features = c(""disp"", ""cyl""), 
           response = ""mpg"")
}

cols <- feature_columns(
  column_numeric(""disp""),
  column_numeric(""cyl"")
)

cols <- feature_columns( 
  column_numeric(""disp"", ""cyl"")
)

model <- linear_regressor(feature_columns = cols)

indices <- sample(1:nrow(mtcars), size = 0.80 * nrow(mtcars))
train <- mtcars[indices, ]
test  <- mtcars[-indices, ]

# train the model
model %>% train(mtcars_input_fn(train))

model %>% evaluate(mtcars_input_fn(test))

new_obs <- mtcars[1:3, ]
model %>% predict(mtcars_input_fn(new_obs))
"
"https://raw.githubusercontent.com/nadizan/test-hugo/18151ccf7c8a07076f486822005dce4b7bf96950/content/post/2018-05-23_ropensci_greta.Rmd","library(greta)

# data
N <- letters[1:8]
treatment_effects <- c(28.39, 7.94, -2.75 , 6.82, -0.64, 0.63, 18.01, 12.16)
treatment_stddevs <- c(14.9, 10.2, 16.3, 11.0, 9.4, 11.4, 10.4, 17.6)

# variables and priors
avg_effect <- normal(mean = 0, sd = 10)
avg_stddev <- normal(5, 1)
school_effects_standard <- normal(0, 1, dim = length(N))
school_effects <- avg_effect + exp(avg_stddev) * school_effects_standard

# likelihood
distribution(treatment_effects) <- normal(school_effects, treatment_stddevs)

# defining the hierarchical model
m <- model(avg_effect, avg_stddev, school_effects_standard)
m

plot(m)

# plotting
DiagrammeR::render_graph(plot(m))

# sampling
draws <- greta::mcmc(m)
plot(draws)

library(reticulate)
tf <- import(""tensorflow"")
tfp <- import(""tensorflow_probability"")
ed <- tfp$edward2

sessionInfo()
"
